<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Topicalizer by LeandroOrdonez</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">Topicalizer</h1>
        <p class="header">Automatic Web API Categorization and Annotation</p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/LeandroOrdonez/Topicalizer/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/LeandroOrdonez/Topicalizer/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/LeandroOrdonez/Topicalizer">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/LeandroOrdonez">LeandroOrdonez</a></p>


      </header>
      <section>
        <h1>
<a name="topicalizer" class="anchor" href="#topicalizer"><span class="octicon octicon-link"></span></a>Topicalizer</h1>

<p>Topicalizer is a tool that allow you to process a bunch of SOAP API descriptors
in order to group the technical information they contain, in semantic related
categories, and specifying this categorization as RDF statements stored in a Sesame
triple-store. As a first step in the process of categorization, this tool applies 
text processing procedures over the service descriptors for extracting some relevant
technical information (operations, documentation and datatypes). Once such information
is available, the tool fits a probabilistic topic model, known as Online LDA, for
infering a set of relevant categories (topics--distributions over terms in a fixed 
vocabulary), and associate a probability distribution over such topics for each one 
of the service operations processed by the tool.</p>

<p>The Online LDA processing is based on the implementation of <code>ONLINE VARIATIONAL 
BAYES FOR LATENT DIRICHLET ALLOCATION</code> by Matthew D. Hoffman, which uses the online 
Variational Bayes (VB) algorithm presented in the paper "Online Learning for Latent 
Dirichlet Allocation" by Matthew D. Hoffman, David M. Blei, and Francis Bach.</p>

<p>The algorithm uses stochastic optimization to maximize the variational
objective function for the Latent Dirichlet Allocation (LDA) topic model.
It only looks at a subset of the total corpus of documents (namely, text files
whose content has been extracted from Web APIs documentation archives) 
each iteration, and thereby is able to find a locally optimal setting of
the variational posterior over the topics more quickly than a batch
VB algorithm could for large corpora.</p>

<h2>
<a name="filesdirectories-provided" class="anchor" href="#filesdirectories-provided"><span class="octicon octicon-link"></span></a>Files/Directories provided:</h2>

<ul>
<li>
<code>lib/</code>: Java Dependencies.</li>
<li>
<code>onlinelda/</code>: Folder containing the Python scripts which uses online VB for LDA to analyze 
the information that has been extracted from SOAP API descriptors.</li>
<li>
<code>outcome/</code>: This folder holds the .txt and .csv files containing the results of
applying Online LDA (<code>topics.&lt;txt/csv&gt;</code> and <code>per-document-topics.&lt;txt/csv&gt;</code>).</li>
<li>
<code>sesame_war/</code>: Folder containing deployable distribution of the <code>Sesame Framework</code>
Server (<code>openrdf-sesame.war</code> and <code>openrdf-workbench.war</code>). </li>
<li>
<code>run.sh</code>: Execution script.</li>
<li>
<code>sample-service-uris.txt</code>: File containing a list of 80 service descriptors available online
(used as a sample input for the tool).</li>
<li>
<code>WebAPIDocProcessing.jar</code> Java App for performing parsing and text processing operations
on the service descriptors.</li>
<li>
<code>README.md</code>: This file.</li>
</ul>

<p>You will need to have the numpy and scipy packages installed somewhere
that Python can find them to use these scripts.</p>

<h2>
<a name="system-requirements" class="anchor" href="#system-requirements"><span class="octicon octicon-link"></span></a>System Requirements:</h2>

<ul>
<li>
<code>Java 1.6.x</code> or greater.</li>
<li><code>MySQL 5.5.x</code></li>
<li>
<code>Apache Tomcat 7.x</code> (or any available servlet container, listening at 8080 port).</li>
</ul>

<h2>
<a name="initial-settings" class="anchor" href="#initial-settings"><span class="octicon octicon-link"></span></a>Initial Settings</h2>

<ol>
<li>In MySQL create a Database with name <code>service_registry</code>.</li>
<li>Deploy both of the Sesame Framework .war files on your servlet container.
After you have deployed the Sesame Server webapp, you should be able to access it, by
default, at path <code>/openrdf-sesame</code> (<code>/openrdf-sesame/home/overview.view</code> for
Apache Tomcat 7).</li>
<li>Create a new <code>Native Java Store</code> with ID <code>WebAPIModel</code> in the Sesame Server, by 
accessing http://localhost:8080/openrdf-workbench/ -&gt; <code>New repository</code>.</li>
<li>
<p>Give execution permissions on the <code>run.sh</code> script. Open a terminal and type:</p>

<pre><code>$chmod u+x run.sh
</code></pre>

<h2>
<a name="running" class="anchor" href="#running"><span class="octicon octicon-link"></span></a>Running</h2>
</li>
<li>
<p>Open a terminal and type <code>./run.sh</code> followed by the path of a text file containing the
list of service descriptor URIs. You could use the <code>sample-service-uris.txt</code> provided
with the tool.</p>

<pre><code>$./run.sh sample-service-uris.txt
</code></pre>
</li>
</ol>

<ul>
<li>
<p>After running the whole process, you could verify that the Sesame store you have created
has been populated with RDF statements corresponding to the categorization extracted by 
running the Online LDA algorithm. These RDF statements instantiate the Classes and
Properties defined in the RDF Schema model available at <code>onlinelda/rdf_sesame/web_api_model.rdf</code>.
Additionally, the categorization results are also available as .txt and .csv files 
at the <code>outcome/</code> foder:</p>

<ul>
<li><p><code>per-document-topics.&lt;txt/csv&gt;</code>: distribution over topics for each one of the processed 
operations.</p></li>
<li><p><code>topics.&lt;txt/csv&gt;</code>: distributions over terms for each one of the topics extracted.</p></li>
</ul>
</li>
</ul>
      </section>
      <footer>
        <p><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		
  </body>
</html>
